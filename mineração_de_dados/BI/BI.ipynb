{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao Business Intelligence e Analytics (BI&A)\n",
    "\n",
    "### 1.1  - Conceitos básicos e terminologias\n",
    "* ``Business Intelligence (BI)``: Processo de coleta, armazenamento, análise e transformação de dados brutos em informações úteis para tomada de decisões.\n",
    "\n",
    "* ``Business Analytics (BA)``: Aplicação de técnicas estatísticas, computacionais e de aprendizado de máquina para analisar e interpretar dados, identificar padrões e tendências, e gerar insights de negócios.\n",
    "* ``Data Warehouse``: Repositório centralizado de dados de uma organização, projetado para facilitar a análise e geração de relatórios.\n",
    "* ``Data Mart``: Subconjunto de um data warehouse focado em uma área específica de negócio.\n",
    "\n",
    "### 1.2 - Importância e benefícios do BI&A\n",
    "* ``Melhor tomada de decisões``: BI&A oferece insights e informações baseadas em dados para apoiar decisões mais eficazes e informadas.\n",
    "\n",
    "* ``Maior eficiência operacional``: A análise de dados pode revelar áreas de melhoria e otimização nos processos de negócios.\n",
    "* ``Aumento da competitividade``: O uso de BI&A pode ajudar as empresas a identificar oportunidades e ameaças no mercado e se posicionar de forma mais competitiva.\n",
    "* ``Melhoria na satisfação do cliente``: A análise de dados permite compreender melhor as necessidades e preferências dos clientes, permitindo oferecer produtos e serviços mais personalizados.\n",
    "\n",
    "### 1.3 - Diferença entre Business Intelligence e Business Analytics\n",
    "\n",
    "``Business Intelligence (BI)``:\n",
    "\n",
    "Focado em análises descritivas e diagnósticas.\n",
    "Responde às perguntas \"o que aconteceu?\" e \"por que aconteceu?\"\n",
    "Relatórios históricos e dashboards de desempenho.\n",
    "Exemplos de ferramentas: Tableau, Power BI, QlikView.\n",
    "\n",
    "``Business Analytics (BA)``:\n",
    "\n",
    "Focado em análises preditivas e prescritivas.\n",
    "Responde às perguntas \"o que vai acontecer?\" e \"o que devemos fazer?\"\n",
    "Modelagem preditiva, análise de cenários e otimização.\n",
    "Exemplos de ferramentas: Python, R, SAS, Apache Spark.\n",
    "\n",
    "### 1.4 - Conclusão\n",
    "\n",
    "BI&A são fundamentais para apoiar a tomada de decisões baseada em dados e impulsionar o sucesso dos negócios.\n",
    "\n",
    "BI e BA são complementares: BI ajuda a entender o passado e o presente, enquanto BA busca prever e influenciar o futuro.\n",
    "O domínio desses conceitos e técnicas é essencial para profissionais que desejam trabalhar com análise de dados e gestão de negócios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Componentes do BI&A\n",
    "\n",
    "### 2.1 ETL (Extração, Transformação e Carregamento)\n",
    "\n",
    "* ``Extração``: Coleta de dados de várias fontes (sistemas de gestão, bases de dados, arquivos, etc.).\n",
    "\n",
    "* ``Transformação``: Processamento e limpeza dos dados extraídos para garantir qualidade, consistência e adequação ao propósito de análise.\n",
    "* ``Carregamento``: Inserção dos dados transformados no Data Warehouse ou Data Mart para armazenamento e análise.\n",
    "\n",
    "Importância do ETL: Garante a disponibilidade, integridade e qualidade dos dados para análise e tomada de decisões.\n",
    "\n",
    "Exemplos de ferramentas ETL: Talend, Microsoft SQL Server Integration Services, Apache NiFi.\n",
    "\n",
    "\n",
    "### 2.2 - Data Warehouse e Data Marts\n",
    "\n",
    "``Data Warehouse``:\n",
    "\n",
    "Repositório centralizado de dados de uma organização.\n",
    "Armazena dados históricos e atuais de várias fontes.\n",
    "Facilita a análise e geração de relatórios.\n",
    "Estruturado em esquemas como o Star Schema e o Snowflake Schema.\n",
    "\n",
    "``Data Mart``:\n",
    "\n",
    "Subconjunto de um Data Warehouse focado em uma área específica de negócio.\n",
    "Permite análises mais rápidas e eficientes para departamentos ou equipes específicas.\n",
    "Pode ser construído de forma independente ou a partir de um Data Warehouse existente.\n",
    "\n",
    "\n",
    "### 2.3 - Ferramentas de visualização e relatórios\n",
    "\n",
    "* Objetivo: Apresentar informações e insights gerados a partir dos dados em formatos visualmente atraentes e facilmente compreensíveis (gráficos, tabelas, mapas, etc.).\n",
    "\n",
    "* Importância: Facilita a comunicação e a compreensão dos resultados das análises de dados por tomadores de decisão e stakeholders.\n",
    "* Exemplos de ferramentas de visualização e relatórios:\n",
    "    * Tableau: Ferramenta de visualização de dados interativa e fácil de usar, com recursos avançados de análise e integração de dados.\n",
    "    \n",
    "    * Microsoft Power BI: Plataforma de BI que oferece visualizações interativas e recursos de análise integrados com o Microsoft Office.\n",
    "    * QlikView: Solução de BI que permite a criação de visualizações personalizadas e relatórios interativos.\n",
    "Slide 5: Conclusão\n",
    "\n",
    "Os componentes do BI&A, como ETL, Data Warehouse e Data Marts, e ferramentas de visualização e relatórios, são essenciais para a construção de uma solução de BI&A eficiente e eficaz.\n",
    "\n",
    "Conhecer e entender esses componentes permite que os profissionais de BI&A selecionem e implementem as melhores soluções e práticas para suas necessidades de negócio."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Introdução ao Python para BI&A\n",
    "\n",
    "### 3.1 - Por que Python?\n",
    "\n",
    "* Linguagem de programação popular, versátil e fácil de aprender.\n",
    "\n",
    "* Grande ecossistema de bibliotecas e módulos para análise de dados, visualização e aprendizado de máquina.\n",
    "* Ampla comunidade e suporte, com muitos recursos, tutoriais e exemplos disponíveis.\n",
    "* Python é uma escolha popular em empresas e instituições acadêmicas para BI&A devido à sua flexibilidade e capacidade de integração com outras tecnologias.\n",
    "\n",
    "### 3.2 - Bibliotecas Python para BI&A\n",
    "\n",
    "* Pandas: Biblioteca para manipulação e análise de dados, oferece estruturas de dados flexíveis e eficientes (DataFrame e Series).\n",
    "\n",
    "* NumPy: Biblioteca para computação numérica, oferece suporte para arrays multidimensionais e funções matemáticas de alto nível.\n",
    "* Matplotlib: Biblioteca para criação de gráficos e visualizações 2D e 3D.\n",
    "* Seaborn: Biblioteca de visualização de dados baseada no Matplotlib, oferece uma interface de alto nível para criar gráficos estatísticos mais atraentes e informativos.\n",
    "* Scikit-learn: Biblioteca para aprendizado de máquina e análise preditiva, oferece uma ampla gama de algoritmos e ferramentas para modelagem e validação de dados.\n",
    "\n",
    "### 3.3 - Demonstração de importação e manipulação de dados com Pandas\n",
    "\n",
    "Objetivo: Mostrar um exemplo prático de como usar o Pandas para importar e manipular dados em Python.\n",
    "\n",
    "Exemplo de código:\n",
    "\n",
    "Este exemplo analisa o desempenho das vendas por categoria de produto e gênero do cliente, fornecendo insights mais detalhados sobre os padrões de compra. Além disso, calculamos a média do lucro por item vendido em cada categoria de produto, o que pode ser útil para entender a rentabilidade de diferentes produtos. Ao final, criamos uma tabela dinâmica (tabela_dinamica) para facilitar a visualização e análise dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas\n",
    "import pandas as pd\n",
    "\n",
    "# Ler arquivo CSV e criar um DataFrame\n",
    "url = \"https://raw.githubusercontent.com/ine-rmotr-curriculum/FreeCodeCamp-Pandas-Real-Life-Example/master/data/sales_data.csv\"\n",
    "dados_vendas = pd.read_csv(url, parse_dates=[\"Date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar as primeiras linhas do DataFrame\n",
    "dados_vendas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar coluna 'Mês' para agrupar vendas por mês\n",
    "dados_vendas['Mês'] = dados_vendas['Date'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_vendas['Date'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_vendas['Date'].dt.to_period('M').head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o lucro médio por item e adicionar como uma nova coluna\n",
    "dados_vendas['Lucro_por_Item'] = dados_vendas['Profit'] / dados_vendas['Order_Quantity']\n",
    "\n",
    "# Agrupar os dados por 'Mês', 'Product_Category' e 'Customer_Gender'\n",
    "# Calcular a soma da receita, lucro e quantidade de pedidos e a média do lucro por item\n",
    "dados_agrupados = dados_vendas.groupby(['Mês', 'Product_Category', 'Customer_Gender']).agg({\n",
    "    'Revenue': 'sum',\n",
    "    'Profit': 'sum',\n",
    "    'Order_Quantity': 'sum',\n",
    "    'Lucro_por_Item': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Criar uma tabela dinâmica (pivot table) com o desempenho das vendas\n",
    "# por categoria de produto e gênero do cliente\n",
    "tabela_dinamica = dados_agrupados.pivot_table(\n",
    "    values=['Revenue', 'Lucro_por_Item'],\n",
    "    index=['Mês', 'Product_Category'],\n",
    "    columns='Customer_Gender',\n",
    "    aggfunc='sum'\n",
    ")\n",
    "\n",
    "# Salvar DataFrame como um novo arquivo CSV\n",
    "tabela_dinamica.to_csv(\"resultado_vendas_categoria_genero.csv\")\n",
    "\n",
    "tabela_dinamica"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Conclusão\n",
    "\n",
    "Python é uma linguagem poderosa e versátil para BI&A, com um ecossistema rico e uma comunidade ativa.\n",
    "A familiaridade com bibliotecas como Pandas, NumPy, Matplotlib e Seaborn é essencial para realizar análises de dados eficientes e eficazes com Python.\n",
    "\n",
    "A prática constante e a exploração de recursos disponíveis são fundamentais para desenvolver habilidades em Python para BI&A."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Análise Exploratória de Dados (EDA) com Python (20 minutos)\n",
    "\n",
    "### 4.1 - Conceito de EDA e sua importância\n",
    "\n",
    "A Análise Exploratória de Dados (EDA) é uma abordagem para analisar conjuntos de dados a fim de resumir suas características principais, geralmente com métodos visuais. É uma etapa crucial na análise de dados, pois ajuda a entender a estrutura, as relações e os padrões nos dados antes de aplicar técnicas de modelagem estatística ou aprendizado de máquina. A EDA é fundamental para identificar problemas nos dados, como valores ausentes, outliers e erros, bem como para gerar hipóteses e extrair insights que podem ser úteis no processo de tomada de decisão.\n",
    "\n",
    "### 4.2 - Técnicas de EDA (estatísticas descritivas, visualizações gráficas, etc.)\n",
    "\n",
    "As técnicas de EDA podem ser divididas em dois tipos principais: estatísticas descritivas e visualizações gráficas.\n",
    "\n",
    "Estatísticas descritivas: São medidas numéricas que resumem e descrevem as características dos dados. Incluem medidas de tendência central (média, mediana e moda), medidas de dispersão (amplitude, variância e desvio padrão) e medidas de forma (assimetria e curtose). Essas estatísticas ajudam a entender a distribuição, a dispersão e a forma dos dados.\n",
    "\n",
    "Visualizações gráficas: São representações visuais dos dados que facilitam a compreensão das relações, padrões e tendências nos dados. Alguns exemplos de gráficos utilizados na EDA são histogramas, gráficos de dispersão, boxplots e gráficos de barras. As visualizações gráficas são complementares às estatísticas descritivas e podem fornecer insights que não são facilmente obtidos apenas com números.\n",
    "\n",
    "### 4.3 - Demonstração prática de EDA com Python (Pandas, Matplotlib e Seaborn)\n",
    "\n",
    "Nesta demonstração, vamos explorar a base de dados de vendas já criada, realizando uma análise exploratória de dados completa utilizando as bibliotecas Pandas, Matplotlib e Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas e carregar os dados\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/ine-rmotr-curriculum/FreeCodeCamp-Pandas-Real-Life-Example/master/data/sales_data.csv\"\n",
    "dados_vendas = pd.read_csv(url, parse_dates=[\"Date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar informações básicas do conjunto de dados\n",
    "# Verificar as primeiras linhas\n",
    "dados_vendas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não acho a melhor forma de ver, mas mostra todas as colunas\n",
    "print(dados_vendas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar o tamanho do conjunto de dados\n",
    "print(dados_vendas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar os tipos de dados e informações gerais\n",
    "print(dados_vendas.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo estatístico das colunas numéricas\n",
    "dados_vendas.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo estatístico das colunas textuais\n",
    "dados_vendas.describe(include = 'O').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular a média da receita por categoria de produto\n",
    "media_receita_categoria = dados_vendas.groupby(\"Product_Category\")[\"Revenue\"].mean()\n",
    "media_receita_categoria"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este gráfico mostra a distribuição das idades dos clientes no conjunto de dados. Ele permite visualizar a frequência de cada faixa etária e ajuda a entender a demografia dos clientes. Podemos identificar em quais faixas etárias os clientes estão mais concentrados e adaptar nossas estratégias de marketing e vendas de acordo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma da coluna 'Customer_Age'\n",
    "sns.histplot(data=dados_vendas, x='Customer_Age', bins=30, kde=True)\n",
    "plt.title('Distribuição da Idade dos Clientes')\n",
    "plt.xlabel('Idade')\n",
    "plt.ylabel('Frequência')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O gráfico de barras exibe a quantidade de vendas para cada categoria de produto. A partir deste gráfico, podemos identificar quais categorias de produtos são mais populares e vendem mais. Isso pode nos ajudar a ajustar nosso estoque e focar nas categorias com maior demanda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de barras da quantidade de vendas por categoria de produto\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=dados_vendas, x='Product_Category', order=dados_vendas['Product_Category'].value_counts().index)\n",
    "plt.title('Quantidade de Vendas por Categoria de Produto')\n",
    "plt.xlabel('Categoria de Produto')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O gráfico de dispersão mostra a relação entre a quantidade de pedidos e a receita para cada categoria de produto. Ele nos ajuda a identificar possíveis padrões e tendências nas vendas e a analisar a eficiência das diferentes categorias de produtos em termos de geração de receita. Com base nesses insights, podemos ajustar nossas estratégias de vendas e estoque para otimizar o desempenho das categorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de dispersão da quantidade de pedidos versus receita\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(data=dados_vendas, x='Order_Quantity', y='Revenue', hue='Product_Category')\n",
    "plt.title('Quantidade de Pedidos vs. Receita por Categoria de Produto')\n",
    "plt.xlabel('Quantidade de Pedidos')\n",
    "plt.ylabel('Receita')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O boxplot apresenta a distribuição da receita por categoria de produto, mostrando a mediana, quartis e possíveis outliers. Ele nos permite comparar rapidamente as categorias em termos de receita e identificar quais delas estão gerando mais lucro. Isso pode nos ajudar a direcionar nossos esforços de marketing e vendas para as categorias mais lucrativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot da receita por categoria de produto\n",
    "# Aplicar a função 'clip' para limitar os valores da coluna 'Revenue' entre 0 e 15000\n",
    "y = dados_vendas['Revenue'].clip(lower=0, upper=5000)\n",
    "\n",
    "# Configurar o tamanho e o estilo do gráfico (opcional)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Criar o boxplot da receita por categoria de produto\n",
    "sns.boxplot(data=dados_vendas, x='Product_Category', y=y)\n",
    "plt.title('Distribuição da Receita por Categoria de Produto')\n",
    "plt.xlabel('Categoria de Produto')\n",
    "plt.ylabel('Receita')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O gráfico de violino é semelhante ao boxplot, mas também mostra a densidade estimada de probabilidade dos dados em diferentes valores. Isso fornece uma visão mais detalhada da distribuição da receita por categoria de produto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar o tamanho e o estilo do gráfico (opcional)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Criar o gráfico de violino da receita por categoria de produto\n",
    "sns.violinplot(data=dados_vendas, x='Product_Category', y='Revenue')\n",
    "plt.title('Distribuição da Receita por Categoria de Produto')\n",
    "plt.xlabel('Categoria de Produto')\n",
    "plt.ylabel('Receita')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O heatmap de correlação exibe a força e a direção das relações entre as variáveis numéricas no conjunto de dados. Ele nos ajuda a identificar quais variáveis estão fortemente relacionadas e quais não estão. Isso pode ser útil para descobrir insights importantes, como a relação entre o custo dos produtos e a receita gerada. Essas informações podem nos ajudar a tomar decisões informadas sobre preços, promoções e outras estratégias de negócios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap de correlação entre as variáveis numéricas\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(dados_vendas.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlação entre Variáveis Numéricas')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Modelagem e Algoritmos de Analytics\n",
    "\n",
    "### 5.1 - Tipos de análises (descritiva, preditiva e prescritiva)\n",
    "* ``Análise Descritiva``: Esta análise busca entender o passado e descrever o que aconteceu nos dados. Envolve o uso de estatísticas descritivas e visualizações gráficas para identificar tendências, padrões e relações nos dados.\n",
    "\n",
    "* ``Análise Preditiva``: A análise preditiva visa prever eventos futuros com base em dados históricos. Ela utiliza algoritmos de aprendizado de máquina e estatísticas para criar modelos que possam fazer previsões sobre novos dados.\n",
    "\n",
    "* ``Análise Prescritiva``: A análise prescritiva busca recomendar ações a serem tomadas com base nas análises descritiva e preditiva. Ela utiliza técnicas de otimização e simulação para identificar as melhores decisões a serem tomadas em diferentes cenários.\n",
    "\n",
    "### 5.2 - Algoritmos de aprendizado de máquina para analytics\n",
    "Alguns dos algoritmos de aprendizado de máquina comumente usados em analytics incluem:\n",
    "\n",
    "* `` Regressão Linear``: para prever valores numéricos com base em variáveis independentes.\n",
    "\n",
    "* ``Regressão Logística``: para prever a probabilidade de um evento binário ocorrer.\n",
    "* ``Árvores de Decisão e Florestas Aleatórias``: para classificação e regressão.\n",
    "* ``K-means e K-medianas``: para agrupamento (clustering) de dados.\n",
    "* ``SVM (Support Vector Machines)``: para classificação e regressão.\n",
    "* ``Redes Neurais Artificiais``: para classificação, regressão e agrupamento.\n",
    "\n",
    "\n",
    "### 5.3 - Demonstração prática de implementação de algoritmos com Python (Scikit-learn)\n",
    "Vamos utilizar a base de dados de exemplo para prever a receita com base nas variáveis disponíveis. Para simplificar, utilizaremos a Regressão Linear. Primeiro, vamos preparar os dados e, em seguida, treinar e avaliar o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparar dados para colocar no modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ine-rmotr-curriculum/FreeCodeCamp-Pandas-Real-Life-Example/master/data/sales_data.csv\"\n",
    "\n",
    "dados_vendas = pd.read_csv(url)\n",
    "\n",
    "# Selecionar as colunas de interesse\n",
    "X = dados_vendas[['Customer_Age', 'Order_Quantity', 'Unit_Cost', 'Unit_Price']].values\n",
    "y = dados_vendas['Revenue'].values\n",
    "\n",
    "# Escalonamento dos dados\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Remodelar y para um array 2D\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Escalonar y usando MinMaxScaler\n",
    "scaler_y = MinMaxScaler()\n",
    "y = scaler_y.fit_transform(y)\n",
    "\n",
    "# Dividir os dados em conjuntos de treinamento e teste - 70% para treinamento\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Dividir os dados em conjuntos de teste e validação - 15% para cada\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``input_dim``: Define o número de entradas (número de recursos) na camada de entrada.\n",
    "\n",
    "``activation``: Define a função de ativação para a camada (exemplos: 'relu', 'sigmoid', 'tanh', 'linear').\n",
    "\n",
    "``optimizer``: Define o otimizador usado para ajustar os pesos da rede neural durante o treinamento (exemplos: 'Adam', 'SGD', 'RMSprop').\n",
    "\n",
    "``loss``: Define a função de perda usada para medir o desempenho do modelo durante o treinamento.\n",
    "\n",
    "``metrics``: Define as métricas adicionais a serem calculadas durante o treinamento.\n",
    "\n",
    "``Dense``(units, activation, kernel_regularizer): Adiciona uma camada densa (totalmente conectada) à rede neural. O parâmetro units especifica o número de neurônios na camada, activation define a função de ativação a ser usada e kernel_regularizer aplica a regularização L1 e L2 aos pesos da camada.\n",
    "\n",
    "``Dropout(rate)``: Adiciona uma camada de dropout à rede neural. O parâmetro rate especifica a fração de neurônios a serem desativados durante o treinamento, o que ajuda a evitar o overfitting.\n",
    "\n",
    "``Adam(learning_rate)``: Define o otimizador Adam com uma taxa de aprendizado (learning_rate) específica. O otimizador Adam é uma variação do método de descida do gradiente estocástico que ajusta adaptativamente a taxa de aprendizado.\n",
    "\n",
    "``model.compile(loss, optimizer, metrics)``: Compila o modelo, especificando a função de perda (loss) a ser minimizada, o otimizador a ser usado e as métricas adicionais para avaliar o desempenho do modelo.\n",
    "\n",
    "``EarlyStopping(monitor, patience, restore_best_weights)``: Define o critério de parada antecipada, que interrompe o treinamento se a métrica monitorada (monitor) não melhorar após um determinado número de épocas (patience). Se restore_best_weights for True, os pesos da melhor época do modelo encontrado serão restaurados.\n",
    "\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs, batch_size, validation_data, callbacks, verbose) treina o modelo usando os dados de treinamento (X_train, y_train) com os seguintes argumentos:\n",
    "\n",
    "``epochs``: número de vezes que o modelo irá iterar sobre todo o conjunto de dados de treinamento.\n",
    "\n",
    "``batch_size``: número de amostras por atualização de gradiente. O padrão é 32.\n",
    "\n",
    "``validation_data``: uma tupla (X_val, y_val) que contém os dados de validação usados para avaliar a perda e as métricas em cada época. Esses dados não são usados para treinar o modelo.\n",
    "\n",
    "``callbacks``: lista de funções de retorno de chamada (callbacks) do Keras a serem aplicadas durante o treinamento. Neste exemplo, estamos usando o EarlyStopping para monitorar a perda de validação e parar o treinamento quando ela não melhorar por um determinado número de épocas (definido pela variável patience).\n",
    "\n",
    "``verbose``: 0, 1 ou 2. Define o modo de exibição do log de treinamento:\n",
    "* 0: modo silencioso (sem saída)\n",
    "* 1: modo de progresso (barra de progresso)\n",
    "* 2: modo de registro por época (uma linha por época)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o modelo\n",
    "model = Sequential()\n",
    "\n",
    "# Adicionar camada densa com 64 neurônios, ativação ReLU e regularização L1 e L2\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)))\n",
    "\n",
    "# Adicionar camada densa com 64 neurônios e ativação ReLU\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# Adicionar dropout de 20% após a camada\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adicionar camada densa com 128 neurônios e ativação ReLU\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Adicionar camada densa com 64 neurônios e ativação ReLU\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Adicionar camada densa de saída com 1 neurônio e ativação linear\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Configurar o otimizador personalizado\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae', 'mse'])\n",
    "\n",
    "# Definir critério de parada antecipada\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Treinar o modelo com parada antecipada\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer previsões no conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_test = scaler_y.inverse_transform(y_test)\n",
    "y_pred = scaler_y.inverse_transform(y_pred)\n",
    "\n",
    "# Avaliar o modelo usando a métrica RMSE (Root Mean Squared Error)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "\n",
    "# Calcular R²\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calcular R² ajustado\n",
    "n = X_test.shape[0]  # número de observações\n",
    "p = X_test.shape[1]  # número de preditores (variáveis independentes)\n",
    "r2_ajustado = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "# Calcular MAE (Mean Absolute Error)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"R²: {r2}\")\n",
    "print(f\"R² ajustado: {r2_ajustado}\")\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``R² (Coeficiente de Determinação)``: Esta métrica indica o quanto do comportamento da variável dependente é explicado pela variável independente no modelo. O valor de R² varia entre 0 e 1, sendo que um valor mais alto indica um melhor ajuste do modelo aos dados.\n",
    "\n",
    "``R² ajustado``: É uma versão modificada do R² que leva em consideração o número de preditores (variáveis independentes) no modelo e o tamanho da amostra. O R² ajustado é útil quando se comparam modelos com diferentes números de preditores, já que ele penaliza modelos mais complexos que não necessariamente melhoram o ajuste.\n",
    "\n",
    "``MAE (Mean Absolute Error)``: É a média das diferenças absolutas entre as previsões e os valores reais. Essa métrica fornece uma ideia da magnitude dos erros, sem considerar a direção dos erros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperar histórico de métricas\n",
    "history_dict = history.history\n",
    "\n",
    "# Definir subplots para perda (loss), erro absoluto médio (mae) e erro médio quadrático (mse)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "# Plotar perda (loss) de treinamento e validação\n",
    "ax1.plot(history_dict['loss'], label='Perda de Treinamento')\n",
    "ax1.plot(history_dict['val_loss'], label='Perda de Validação')\n",
    "ax1.set_title('Perda de Treinamento e Validação')\n",
    "ax1.set_xlabel('Épocas')\n",
    "ax1.set_ylabel('Perda')\n",
    "ax1.legend()\n",
    "\n",
    "# Plotar erro absoluto médio (mae) de treinamento e validação\n",
    "ax2.plot(history_dict['mae'], label='MAE de Treinamento')\n",
    "ax2.plot(history_dict['val_mae'], label='MAE de Validação')\n",
    "ax2.set_title('Erro Absoluto Médio de Treinamento e Validação')\n",
    "ax2.set_xlabel('Épocas')\n",
    "ax2.set_ylabel('Erro Absoluto Médio')\n",
    "ax2.legend()\n",
    "\n",
    "# Plotar erro médio quadrático (mse) de treinamento e validação\n",
    "ax3.plot(history_dict['mse'], label='MSE de Treinamento')\n",
    "ax3.plot(history_dict['val_mse'], label='MSE de Validação')\n",
    "ax3.set_title('Erro Médio Quadrático de Treinamento e Validação')\n",
    "ax3.set_xlabel('Épocas')\n",
    "ax3.set_ylabel('Erro Médio Quadrático')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
